# -*- coding: utf-8 -*-
"""saarthiAI_Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10E_K9XQZoiklY5mC-ek2KL3tN9LMxJ3I
"""

import pandas as pd

import tensorflow as tf

import numpy as np

import yaml
import sys
import pickle

try:
  files = open(sys.argv[sys.argv.index("--config")+1],"r")
except IndexError:
  raise FileNotFoundError
config = yaml.safe_load(files)
files.close()

train_dat = pd.read_csv(config['data_directory']+config['train_data_name'])
valid_dat = pd.read_csv(config['data_directory'] + config['valid_data_name'])

for i in config['drop_columns']:
  train_dat = train_dat.drop(i,1)  
  valid_dat = valid_dat.drop(i,1)

tokenizer_transcription = tf.keras.preprocessing.text.Tokenizer()
tokenizer_action = tf.keras.preprocessing.text.Tokenizer(split = '\n')
tokenizer_object = tf.keras.preprocessing.text.Tokenizer(split = '\n')
tokenizer_location = tf.keras.preprocessing.text.Tokenizer(split = '\n')

tokenizer_transcription.fit_on_texts(train_dat['transcription'])
tokenizer_action.fit_on_texts(train_dat['action'])
tokenizer_object.fit_on_texts(train_dat['object'])
tokenizer_location.fit_on_texts(train_dat['location'])

train_dat['transcription'] = tokenizer_transcription.texts_to_sequences(train_dat['transcription'])
train_dat['action'] = tokenizer_action.texts_to_sequences(train_dat['action'])
train_dat['object'] = tokenizer_object.texts_to_sequences(train_dat['object'])
train_dat['location'] = tokenizer_location.texts_to_sequences(train_dat['location'])

valid_dat['transcription'] = tokenizer_transcription.texts_to_sequences(valid_dat['transcription'])
valid_dat['action'] = tokenizer_action.texts_to_sequences(valid_dat['action'])
valid_dat['object'] = tokenizer_object.texts_to_sequences(valid_dat['object'])
valid_dat['location'] = tokenizer_location.texts_to_sequences(valid_dat['location'])

num_transcript = len(tokenizer_transcription.word_index)
num_action = len(tokenizer_action.word_index)
num_object = len(tokenizer_object.word_index)
num_location = len(tokenizer_location.word_index)

transcription_max_len = config['input_max_len']

X_train = tf.keras.preprocessing.sequence.pad_sequences(train_dat['transcription'],transcription_max_len)
Y1_train = tf.keras.utils.to_categorical([i[0]-1 for i in train_dat['action']],num_action)
Y2_train = tf.keras.utils.to_categorical([i[0]-1 for i in train_dat['object']],num_object)
Y3_train = tf.keras.utils.to_categorical([i[0]-1 for i in train_dat['location']],num_location)

X_val = tf.keras.preprocessing.sequence.pad_sequences(valid_dat['transcription'],transcription_max_len)
Y1_val = tf.keras.utils.to_categorical([i[0]-1 for i in valid_dat['action']],num_action)
Y2_val = tf.keras.utils.to_categorical([i[0]-1 for i in valid_dat['object']],num_object)
Y3_val = tf.keras.utils.to_categorical([i[0]-1 for i in valid_dat['location']],num_location)

with tf.distribute.MirroredStrategy().scope():
  inp = tf.keras.layers.Input(shape=(transcription_max_len))
  l1 = tf.keras.layers.Dense(32,activation = 'relu')(inp)
  l2 = tf.keras.layers.Dense(32,activation = 'relu')(l1)
  l31 = tf.keras.layers.Dense(num_action,'softmax',name = 'action')(l2)
  l32 = tf.keras.layers.Dense(num_object,'softmax',name = 'object')(l1)
  l33 = tf.keras.layers.Dense(num_location,'softmax',name = 'location')(l1)

  model = tf.keras.Model(inputs = [inp],outputs = [l31,l32,l33])

  #print(model.summary())


  model.compile(config['optimizer'],config['loss'],metrics=config['metrics'])

  tb_callback = tf.keras.callbacks.TensorBoard(config['tensorboard_log_directory'],histogram_freq=1)
  history = model.fit(X_train,[Y1_train,Y2_train,Y3_train],epochs = config['epochs'],validation_data=(X_val,[Y1_val,Y2_val,Y3_val]),callbacks = [tb_callback])

model.save(config['model_save_directory'] + config['model_save_name'])

files = open(config['model_save_directory'] + "tokenizer_transcription","wb")
pickle.dump(tokenizer_transcription,files)
files.close()

files = open(config['model_save_directory'] + "tokenizer_action","wb")
pickle.dump(tokenizer_action,files)
files.close()
files = open(config['model_save_directory'] + "tokenizer_object","wb")
pickle.dump(tokenizer_object,files)
files.close()
files = open(config['model_save_directory'] + "tokenizer_location","wb")
pickle.dump(tokenizer_location,files)
files.close()


